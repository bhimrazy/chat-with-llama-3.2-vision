# Chat with Llama 3.2-Vision (11B) Multimodal LLM

<img src="https://github.com/user-attachments/assets/645d4447-eb8a-4992-9c53-8c37e904e82f" class="center-cropped">

## Overview

The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. [Read more](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)

## Features

- Visual recognition
- Image reasoning
- Captioning
- Answering general questions about an image

## Installation

To get started with this project, follow these steps:

1. Clone the repository:
    ```sh
    git clone https://github.com/your-repo.git
    cd your-repo
    ```

2. Install the required dependencies:
    ```sh
    pip install -r requirements.txt
    ```

## Usage

To run the application, execute the following command:
```sh
streamlit run app.py
```

